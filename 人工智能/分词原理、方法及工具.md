# 中文分词的原理、方法与工具

海德格尔说 “词语破碎处，无物可存在”。中文句子不像英文那样的词与词之间有显示空格边界，使得词和词组边界模糊。

为了让计算机更容易理解文本，通常中文信息处理的第一步是中文分词。中文分词是在中文句子中的词与词之间加上边界标记。

本文首先介绍词、词组、句子、语言模型等基本概念及基本原理，比如：短语结构语法（PSG）模型、n 元语法模型（ n-gram）、神经网络语言模型（NNLM）、Masked Language Model（MLM）；

接着介绍主要中文分词方法，比如最短路径分词、n 元语法分词、由字构词分词、循环神经网络分词、Transformer 分词；

然后介绍当前主要使用的分词工具，比如 jieba、HanLP、FoolNLTK；

最后抛出个人认为垂直领域如何中文分词及发展趋势。

文章目录如下：

一、中文分词原理  
1、中文分词  
2、词、词组、句子  
3、语言模型  
4、中文分词发展简史  
二、中文分词方法  
1、最短路径分词  
2、n 元语法分词  
3、由字构词分词  
4、循环神经网络分词  
5、Transformer 分词  
三、中文分词工具  
1、jieba  
2、HanLP  
3、FoolNLTK  
四、总结  
1、规则 VS 统计 VS 深度  
2、垂直领域中文分词  
3、中文分词发展趋势

直接上 PPT

![](https://pic3.zhimg.com/v2-22d220efee9c9f57c0ad1dfae1890312_r.jpg)

为什么要中文分词？

![](https://pic4.zhimg.com/v2-bf6c8e73cf2a45b50642cc5af208b9e7_r.jpg)

## **一、中文分词原理**

![](https://pic4.zhimg.com/v2-8a9670c038d00e90f74a1d35e9f672cf_r.jpg)

**1、中文分词**

什么是中文分词？

给出定义：中文分词是在中文句子中的词与词之间加上边界标记。

![](https://pic1.zhimg.com/v2-22575864f92bd22dd1ce14f6f0ad1328_r.jpg)

中文分词总的来说就两种方法：一种是由句子到词；另一种是由字到词。

![](https://pic1.zhimg.com/v2-a34e42b671691683276e131f81aeaec0_r.jpg)

中文分词本质：划分词的边界

![](https://pic2.zhimg.com/v2-325454fca5573e8794c6a76d6860ce21_r.jpg)

同时，中文分词也面临着分词规范、歧义切分、新词识别等挑战。

![](https://pic3.zhimg.com/v2-99e409e4a7a0225f9ab700e267ea6f5e_r.jpg)

**2、词、词组、句子**

什么是词？什么是词组？什么是句子？

搞懂这些基本概念，更容易处理它们。

![](https://pic1.zhimg.com/v2-09f3383492d835e1f6325b8fb07b99b8_r.jpg)

**3、语言模型**

什么是语言模型？

由语音、词汇、语法构成的交流模型。

![](https://pic4.zhimg.com/v2-a9e8da8e47303bdbc57bf1fe63ec4f3b_r.jpg)

短语结构语法（ Phrase Structure Grammar， PSG）

![](https://pic4.zhimg.com/v2-ef23fefc102f27635bb6db5daceddce7_r.jpg)

**n 元语法模型（** **n-gram）**

![](https://pic1.zhimg.com/v2-3f87882a86a5771f84f2cd69ff63918c_r.jpg)

常见的 n 元语法模型如下表所示：

![](https://pic3.zhimg.com/v2-89dcf1fe7e39ba99dd7be700ba0269b6_r.jpg)

神经网络语言模型（NNLM）

![](https://pic1.zhimg.com/v2-ce668dfa0e408ae77b30cd707f1b6198_r.jpg)

Masked Language Model（MLM）

![](https://pic4.zhimg.com/v2-b7380562358c0320ed024a6e81187bff_r.jpg)

**4、中文分词发展简史**

![](https://pic3.zhimg.com/v2-641aab32d832a5ca47b785d2f629bda2_r.jpg)

## **二、中文分词方法**

中文分词代表方法有最短路径分词、n 元语法分词、由字构词分词、循环神经网络分词、Transformer 分词等。

![](https://pic2.zhimg.com/v2-e5e073bc7eb40dbab4716610cf5dd17d_r.jpg)

**1、最短路径分词**

![](https://pic3.zhimg.com/v2-6c4dd527f05f42e1b08597cd38be231e_r.jpg)

**2、n 元语法分词**

![](https://pic1.zhimg.com/v2-d0da18f10d89e4d5f36dff88d6521f70_r.jpg)

举一个 n 元语法分词的例子。

![](https://pic3.zhimg.com/v2-efc9ae53c1d4702436e37fb806842452_r.jpg)

**3、由字构词分词**

![](https://pic1.zhimg.com/v2-cf7eaf042d0b917d8d3457afbe31c010_r.jpg)

常用的三类由字构词

![](https://pic4.zhimg.com/v2-5ca46630ab2b646d3de9be3cb2c7ae83_r.jpg)

**4、循环神经网络分词**

![](https://pic1.zhimg.com/v2-9230655e5c08b2a303a8722f653e6e04_r.jpg)

循环神经网中文分词有：LSTM、LSTM+CRF、BiLSTM-CRF、LSTM-CNNs-CRF 等。

循环神经网络中文分词的结构图如下：

![](https://pic1.zhimg.com/v2-799349a3095bb99efd779c0e879ed54c_r.jpg)

**5、Transformer 分词**

2014 年，Google 在《Recurrent Models of Visual Attention》论文中提出 Attention 机制。

2017 年，Google 在《Attention is All You Need》论文中提出 Transformer 模型。

![](https://pic2.zhimg.com/v2-fcc7c5ed7e2256ce817e78f9cf821b99_r.jpg)

2019 年，邱锡鹏在《Multi-Criteria Chinese Word Segmentation with Transformer》论文中提出 Transformer 中文分词模型如下图所示：

![](https://pic2.zhimg.com/v2-dc0909e7b398364ca04cfa44f06bdcc5_r.jpg)

Transformer 中文分词**学习结果**如下图所示：

![](https://pic2.zhimg.com/v2-8ac61a1ad2c7738905d04a52d062d60d_r.jpg)

## **三、中文分词工具**

中文分词工具工具很多，这里我们选择使用较多，关注度较高的 jieba、HanLP、FoolNLTK 等来介绍。

![](https://pic2.zhimg.com/v2-9a8db3552dd0bf208fb79c5f9e6b1709_r.jpg)

jieba、HanLP、snownlp、FoolNLTK、LTP、THULAC 等分词工具概览。

![](https://pic2.zhimg.com/v2-b7b882bd4968783edf63f2bc2a0f54b1_r.jpg)

**1、jieba**

jieba 概述

![](https://pic3.zhimg.com/v2-7c90327d41a98c780358204c9c5662d2_r.jpg)

jieba 分词原理：HMM（隐马尔可夫模型）。更多 HMM 内容可参考：

[刘启林：隐马尔可夫模型 HMM 的原理及应用](https://zhuanlan.zhihu.com/p/111899116)

![](https://pic1.zhimg.com/v2-e07339e1e662e06bee2f8feccb99b4c8_r.jpg)

**jieba 中文分词代码实例如下：**

# jieba 0.42.1

import jieba

string = '我喜欢北京冬奥会'

print("，".join(jieba.cut(string)))

**2、HanLP**

HanLP 概述

![](https://pic4.zhimg.com/v2-220720cbf837a717c1bc9a8ef00a8447_r.jpg)

HanLP 实现的基于 CRF 分词原理如下：

![](https://pic3.zhimg.com/v2-676e0f7848c0489cd7474b2959a2ae6a_r.jpg)

HanLP 中文分词代码实例如下：

# HanLP1.7.7

from pyhanlp import *
string = '我喜欢北京冬奥会'

HanLP.Config.ShowTermNature = False

print(HanLP.segment(string))

**3、FoolNLTK**

FoolNLTK 概要

![](https://pic2.zhimg.com/v2-c156c14f6c8dd495feaadafe53524629_r.jpg)

FoolNLTK 分词原理如下：

BiLSTM-CRF 模型架构

![](https://pic1.zhimg.com/v2-e3a60a0e575dd92dc041c99700171a78_r.jpg)

各分词工具对比表如下：

![](https://pic2.zhimg.com/v2-2252aa251786b9cb341ea6ef2c21a7e1_r.jpg)

中文分词工具使用总结如下：

![](https://pic1.zhimg.com/v2-d91a508dfd3a025c941945adfbb14508_r.jpg)

## **四、总结**

![](https://pic4.zhimg.com/v2-5233baf9a861131cda4be9b21d9b7db7_r.jpg)

**1、规则 VS 统计 VS 深度**

基于规则分词、基于统计分词与基于深度学习分词的对比。

![](https://pic2.zhimg.com/v2-edebe2f3f1460aa09d2e0e1ea4a80bd9_r.jpg)

**2、垂直领域中文分词**

垂直领域的中文分词现状与挑战。

![](https://pic3.zhimg.com/v2-886094264d7754ed2a5491d8e40f064a_r.jpg)

**3、中文分词发展趋势**

![](https://pic2.zhimg.com/v2-24c2dcaed4a6ec315761f816fab23bd5_r.jpg)

中文分词呈现两个发展趋势：

1、越来越多的 Attention 方法应用到中文分词上。

2、数据科学与语言科学融合，发挥彼此优势。

由于当前自己的能力和水平的限制，我的可能是错的，或者是片面，这里抛砖引玉，期待与您一起交流探讨。

参考文献：

1、中国社会科学院语言研究所词典编辑室, 现代汉语词典（第 7 版）, 商务印书馆 [M], 2017.01

2、宗成庆, 统计自然语言处理（第 2 版）, 清华大学出版社 [M], 2013.08

3、黄昌宁, 赵海, 由字构词——中文分词新方法, 中国中文信息学会二十五周年学术会议 [J], 2006

4、姜维, 文本分析与文本挖掘, 科学出版社 [M], 2018.12

5、Xipeng Qiu 等, Multi-Criteria Chinese Word Segmentation with Transformer, 2019.06
