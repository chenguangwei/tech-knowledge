awk命令去重，sort, uniq命令去重统计
======


利用Linux命令行进行文本按行去重并按重复次数排序

linux命令行提供了非常强大的文本处理功能，组合利用linux命令能实现好多强大的功能。本文这里举例说明如何利用Linux命令行进行文本按行去重并按重复次数排序。主要用到的命令有sort，uniq和cut。其中，sort主要功能是排序，uniq主要功能是实现相邻文本行的去重，cut可以从文本行中提取相应的文本列(简单地说，就是按列操作文本行)。

用于演示的测试文件内容如下：

Hello World. 
Apple and Nokia. 
Hello World. 
I wanna buy an Apple device. 
The Iphone of Apple company. 
Hello World. 
The Iphone of Apple company. 
My name is Friendfish. 
Hello World. 
Apple and Nokia. 
实现命令及过程如下：

1、文本行去重

(1)排序 
由于uniq命令只能对相邻行进行去重复操作，所以在进行去重前，先要对文本行进行排序，使重复行集中到一起。 
$ sort test.txt 
Apple and Nokia. 
Apple and Nokia. 
Hello World. 
Hello World. 
Hello World. 
Hello World. 
I wanna buy an Apple device. 
My name is Friendfish. 
The Iphone of Apple company. 
The Iphone of Apple company.

(2)去掉相邻的重复行 
$ sort test.txt | uniq 
Apple and Nokia. 
Hello World. 
I wanna buy an Apple device. 
My name is Friendfish. 
The Iphone of Apple company.

2、文本行去重并按重复次数排序 
(1)首先，对文本行进行去重并统计重复次数(uniq命令加-c选项可以实现对重复次数进行统计。)。 
$ sort test.txt | uniq -c 
2 Apple and Nokia. 
4 Hello World. 
1 I wanna buy an Apple device. 
1 My name is Friendfish. 
2 The Iphone of Apple company.

(2)对文本行按重复次数进行排序。 
sort -n可以识别每行开头的数字，并按其大小对文本行进行排序。默认是按升序排列，如果想要按降序要加-r选项(sort -rn)。 
$ sort test.txt | uniq -c | sort -rn 
4 Hello World. 
2 The Iphone of Apple company. 
2 Apple and Nokia. 
1 My name is Friendfish.

(3)每行前面的删除重复次数。 
cut命令可以按列操作文本行。可以看出前面的重复次数占8个字符，因此，可以用命令cut -c 9- 取出每行第9个及其以后的字符。 
$ sort test.txt | uniq -c | sort -rn | cut -c 9- 
Hello World. 
The Iphone of Apple company. 
Apple and Nokia. 
My name is Friendfish. 
I wanna buy an Apple device. 
下面附带说一下cut命令的使用，用法如下：

cut -b list [-n] [file …] 
cut -c list [file …] 
cut -f list [-d delim][-s][file …]

上面的-b、-c、-f分别表示字节、字符、字段（即byte、character、field）； 
list表示-b、-c、-f操作范围，-n常常表示具体数字； 
file表示的自然是要操作的文本文件的名称； 
delim（英文全写：delimiter）表示分隔符，默认情况下为TAB； 
-s表示不包括那些不含分隔符的行（这样有利于去掉注释和标题） 
三种方式中，表示从指定的范围中提取字节（-b）、或字符（-c）、或字段（-f）。

范围的表示方法： 
n 只有第n项 
n- 从第n项一直到行尾 
n-m 从第n项到第m项(包括m) 
-m 从一行的开始到第m项(包括m) 
- 从一行的开始到结束的所有项 
在写这篇文章的时候，用到了vim的大小写转化的快捷键：gu变小写，gU变大写。结合ctrl+v能够将一片文字中的字符进行大小写转换，非常好用。

awk命令去除文件中重复数据的办法

　　首先准备一个文本文件，随便写个文件，包含重复行数据的即可，或者你可以参考我这里的文件：

CodingAnts@ubuntu:~/awk$ cat dup
hello world
awk
coding ants
hello world
awk
hello world
awk
coding ants
coding ants
1
2
3
4
5
6
7
8
9
10
11
　　共有9行，后面6行都是重复的前面的几行，最终的效果应该是只显示上面重点显示的那几行，先来看看效果：

CodingAnts@ubuntu:~/awk$ awk '!a[$0]++' dup
hello world
awk
coding ants
1
2
3
4
5
　　在《awk程序指令模型》中介绍了awk的程序指令由模式和操作组成，即Pattern { Action }的形式，如果省略Action，则默认执行 print $0 的操作。

　　实现去除重复功能的就是这里的Pattern：

!a[$0]++
1
2
　　在awk中，对于未初始化的数组变量，在进行数值运算的时候，会赋予初值0，因此a[$0]=0，++运算符的特性是先取值，后加1，因此Pattern等价于

!0
1
2
　　而0为假，!为取反，因此整个Pattern最后的结果为1，相当于if(1)，Pattern匹配成功，输出当前记录，对于dup文件，前3条记录的处理方式都是如此。

　　当读取第4行数据“hello world”的时候，a[$0]=1，取反后的结果为0，即Pattern为0，Pattern匹配失败，因此不输出这条记录，后续的数据以此类推，最终成功实现去除文件中的重复行